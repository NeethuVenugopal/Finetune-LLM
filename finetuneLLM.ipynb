{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42cfa753-8768-4d08-9f35-2c0bb7a7edba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (23.2.1)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch==1.13.1\n",
      "  Using cached torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "Collecting torchdata==0.5.1\n",
      "  Using cached torchdata-0.5.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.13.1) (4.7.1)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1)\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1)\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1)\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1)\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.7/site-packages (from torchdata==0.5.1) (2.0.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchdata==0.5.1) (2.31.0)\n",
      "Collecting portalocker>=2.0.0 (from torchdata==0.5.1)\n",
      "  Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (65.5.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.41.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.5.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.5.1) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.5.1) (2023.7.22)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: portalocker, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, torchdata\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 portalocker-2.7.0 torch-1.13.1 torchdata-0.5.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9f606f9-9651-4e95-96cd-d7799287bbdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.27.2\n",
      "  Using cached transformers-4.27.2-py3-none-any.whl (6.8 MB)\n",
      "Collecting datasets==2.11.0\n",
      "  Using cached datasets-2.11.0-py3-none-any.whl (468 kB)\n",
      "Collecting evaluate==0.4.0\n",
      "  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "Collecting rouge_score==0.1.2\n",
      "  Using cached rouge_score-0.1.2-py3-none-any.whl\n",
      "Collecting loralib==0.1.1\n",
      "  Using cached loralib-0.1.1-py3-none-any.whl (8.8 kB)\n",
      "Collecting peft==0.3.0\n",
      "  Using cached peft-0.3.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.27.2) (3.0.12)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.27.2)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.11.0 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.27.2) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.27.2) (20.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.27.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.27.2) (2023.6.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.27.2) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.2)\n",
      "  Using cached tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.27.2) (4.42.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.27.2) (6.7.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets==2.11.0) (12.0.1)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.11.0)\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==2.11.0) (1.3.5)\n",
      "Collecting tqdm>=4.27 (from transformers==4.27.2)\n",
      "  Obtaining dependency information for tqdm>=4.27 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets==2.11.0)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/53/d3/01b60fb043086f941b932df1463f9b637c45ecdc846481c331fa31bd7ed1/xxhash-3.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached xxhash-3.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets==2.11.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.7/site-packages (from datasets==2.11.0) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets==2.11.0) (3.8.5)\n",
      "Collecting responses<0.19 (from datasets==2.11.0)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting absl-py (from rouge_score==0.1.2)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from rouge_score==0.1.2) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from rouge_score==0.1.2) (1.14.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from peft==0.3.0) (5.6.7)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from peft==0.3.0) (1.13.1)\n",
      "Collecting accelerate (from peft==0.3.0)\n",
      "  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/10/d3/5382aa337d3e67214003a17b06bfc07cf0334356b4e8aaf3b12b0d38c83f/accelerate-0.20.3-py3-none-any.whl.metadata\n",
      "  Using cached accelerate-0.20.3-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (1.3.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (0.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (4.7.1)\n",
      "Collecting packaging>=20.0 (from transformers==4.27.2)\n",
      "  Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.27.2) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.27.2) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.27.2) (2023.7.22)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch>=1.13.0->peft==0.3.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch>=1.13.0->peft==0.3.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch>=1.13.0->peft==0.3.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch>=1.13.0->peft==0.3.0) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.3.0) (65.5.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.3.0) (0.41.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.27.2) (2.2.0)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.11.0)\n",
      "  Using cached multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->rouge_score==0.1.2) (8.1.6)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->rouge_score==0.1.2) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==2.11.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==2.11.0) (2019.3)\n",
      "Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "Using cached xxhash-3.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tokenizers, xxhash, tqdm, packaging, loralib, dill, absl-py, responses, multiprocess, huggingface-hub, transformers, rouge_score, accelerate, peft, datasets, evaluate\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.42.1\n",
      "    Uninstalling tqdm-4.42.1:\n",
      "      Successfully uninstalled tqdm-4.42.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.1\n",
      "    Uninstalling packaging-20.1:\n",
      "      Successfully uninstalled packaging-20.1\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.15\n",
      "    Uninstalling multiprocess-0.70.15:\n",
      "      Successfully uninstalled multiprocess-0.70.15\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n",
      "spyder 4.0.1 requires pyqt5<5.13; python_version >= \"3\", which is not installed.\n",
      "spyder 4.0.1 requires pyqtwebengine<5.13; python_version >= \"3\", which is not installed.\n",
      "notebook 6.5.5 requires pyzmq<25,>=17, but you have pyzmq 25.1.0 which is incompatible.\n",
      "pathos 0.3.1 requires dill>=0.3.7, but you have dill 0.3.6 which is incompatible.\n",
      "pathos 0.3.1 requires multiprocess>=0.70.15, but you have multiprocess 0.70.14 which is incompatible.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.7 which is incompatible.\n",
      "spyder 4.0.1 requires jedi==0.14.1, but you have jedi 0.18.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-1.4.0 accelerate-0.20.3 datasets-2.11.0 dill-0.3.6 evaluate-0.4.0 huggingface-hub-0.16.4 loralib-0.1.1 multiprocess-0.70.14 packaging-23.1 peft-0.3.0 responses-0.18.0 rouge_score-0.1.2 tokenizers-0.13.3 tqdm-4.66.1 transformers-4.27.2 xxhash-3.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b04715-269a-4bb7-9bd7-0d2f3c5633ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c649ab2-8ace-4cef-b4a3-4b3be17140f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-391706c81424fc80/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d20c38f445342d0b690dcfc54008032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee4204d-c25b-4bf1-9147-61ef6856b86d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype = torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa236105-2d34-4a33-864d-85e05dc8d5fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Trainable model parameters : 247577856 \n",
      " All model parameters : 247577856 \n",
      " Percentage of trainable model parameters : 100.0 \n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\" Trainable model parameters : {trainable_model_params} \\n All model parameters : {all_model_params} \\n Percentage of trainable model parameters : {(trainable_model_params/all_model_params)*100} \"\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0a857c3-115d-40a4-9918-8b8fac7d900a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "EXAMPLE  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Human Dialogue:\n",
      "\n",
      "    summarize the following conversation.\n",
      "    #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "    \n",
      "    summary:\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Machine Summary - Zero Shot:\n",
      "The train is about to leave.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "EXAMPLE  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Human Dialogue:\n",
      "\n",
      "    summarize the following conversation.\n",
      "    #Person1#: OK, that's a cut! Let's start from the beginning, everyone.\n",
      "#Person2#: What was the problem that time?\n",
      "#Person1#: The feeling was all wrong, Mike. She is telling you that she doesn't want to see you any more, but I want to get more anger from you. You're acting hurt and sad, but that's not how your character would act in this situation.\n",
      "#Person2#: But Jason and Laura have been together for three years. Don't you think his reaction would be one of both anger and sadness?\n",
      "#Person1#: At this point, no. I think he would react the way most guys would, and then later on, we would see his real feelings.\n",
      "#Person2#: I'm not so sure about that.\n",
      "#Person1#: Let's try it my way, and you can see how you feel when you're saying your lines. After that, if it still doesn't feel right, we can try something else.\n",
      "    \n",
      "    summary:\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Machine Summary - Zero Shot:\n",
      "The two of you are going to try to figure out how to react to Jason's reaction.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n"
     ]
    }
   ],
   "source": [
    "example_indices = [40,100]\n",
    "dash_line = '-'.join(\"\" for x in range(100))\n",
    "for i,index in enumerate(example_indices):\n",
    "    dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "    summary = dataset[\"test\"][index][\"summary\"]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    summarize the following conversation.\n",
    "    {dialogue}\n",
    "    \n",
    "    summary:\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt,return_tensors = 'pt')\n",
    "    \n",
    "    prediction = original_model.generate(inputs[\"input_ids\"], max_new_tokens = 50)\n",
    "    output = tokenizer.decode(prediction[0],skip_special_tokens = True)\n",
    "    \n",
    "    print(dash_line)\n",
    "    print(\"EXAMPLE \", i+1)\n",
    "    print(dash_line)\n",
    "    print(\"Human Dialogue:\")\n",
    "    print(prompt)\n",
    "    print(dash_line)\n",
    "    print(\"Machine Summary - Zero Shot:\")\n",
    "    print(output)\n",
    "    print(dash_line)\n",
    "    print(\"Human Summary:\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339253be-d284-4881-91b2-ca9497f92460",
   "metadata": {},
   "source": [
    "2. Perform Full Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbc57be-c0d8-4b0a-811c-609126bba970",
   "metadata": {},
   "source": [
    "2.1 Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13062b00-0439-4264-b870-44d375da79a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = \"Summarize the following conversation. \\n\\n\"\n",
    "    end_prompt = \"\\n\\n Summary:\"\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example[\"input_ids\"] = tokenizer(prompt, padding = \"max_length\", truncation = True, return_tensors = \"pt\").input_ids\n",
    "    example[\"labels\"] = tokenizer(example[\"summary\"], padding = \"max_length\", truncation = True, return_tensors = \"pt\").input_ids\n",
    "    return example\n",
    "\n",
    "tokenized_datasets= dataset.map(tokenize_function, batched = True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"id\",\"topic\",\"dialogue\",\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03415061-db75-4237-bd78-3084554dadce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63503e84-b8e3-4f04-aca4-baa4e79080dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of dataset\n",
      "\n",
      "Training set : (125, 2)\n",
      "\n",
      "Training set : (5, 2)\n",
      "\n",
      "Training set : (15, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of dataset\\n\")\n",
    "print(f\"Training set : {tokenized_datasets['train'].shape}\\n\")\n",
    "print(f\"Training set : {tokenized_datasets['validation'].shape}\\n\")\n",
    "print(f\"Training set : {tokenized_datasets['test'].shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d98770d-ee75-40d4-a60a-802e3bfd9f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 125\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "116fe676-e153-485a-a1d1-e27a62fa2a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = f'./dialogue_summary_trainig_{str(int(time.time()))}'\n",
    "training_args = TrainingArguments(\n",
    "            output_dir = output_dir,\n",
    "            learning_rate = 1e-5,\n",
    "            num_train_epochs = 1,\n",
    "            weight_decay = 0.01,\n",
    "            logging_steps = 1,\n",
    "            max_steps = 1)\n",
    "trainer = Trainer(\n",
    "    model = original_model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_datasets[\"train\"],\n",
    "    eval_dataset = tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "963091a5-0ad4-44b0-ba4d-48502407ae73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('full_dialogue_summary_checkpoint_local/tokenizer_config.json',\n",
       " 'full_dialogue_summary_checkpoint_local/special_tokens_map.json',\n",
       " 'full_dialogue_summary_checkpoint_local/tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "full_model_path = f\"full_dialogue_summary_checkpoint_local\"\n",
    "trainer.model.save_pretrained(full_model_path)\n",
    "tokenizer.save_pretrained(full_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01acfbcc-1d05-4038-9094-83db6c1ec260",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal error: An error occurred (IllegalLocationConstraintException) when calling the ListObjectsV2 operation: The unspecified location constraint is incompatible for the region specific endpoint this request was sent to.\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/ ./flan-dialogue-summary-checkpoint/\n",
    "#Did this from cmd . Used the same command without first sign as it is windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11072fd3-4510-425e-8807-5361753d50bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access './flan_dialogue_summary_checkpoint/pytorch_model.bin': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./flan_dialogue_summary_checkpoint/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2be6dc43-e231-490a-a06f-530b56e7356d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./flan-dialogue-summary-checkpoint\", torch_dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d4d9dcc-5577-4baf-8edf-f24411d05acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "EXAMPLE  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Original model summary:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Instruct Model Summary:\n",
      "#Person1# suggests #Person2# adding a painting program to #Person2#'s software and upgrading the hardware. #Person2# also wants to add a CD-ROM drive.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dash_line = '-'.join(\"\" for x in range(100))\n",
    "\n",
    "dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "human_summary = dataset[\"test\"][index][\"summary\"]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "summarize the following conversation.\n",
    "{dialogue}\n",
    "\n",
    "summary:\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt,return_tensors = 'pt').input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids = input_ids, generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1))\n",
    "original_model_text_outputs = tokenizer.decode(original_model_outputs[0],skip_special_tokens = True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids = input_ids, generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1))\n",
    "instruct_model_text_outputs = tokenizer.decode(instruct_model_outputs[0],skip_special_tokens = True)\n",
    "\n",
    "\n",
    "print(dash_line)\n",
    "print(\"EXAMPLE \", i+1)\n",
    "print(dash_line)\n",
    "print(\"Human Summary:\")\n",
    "print(human_summary)\n",
    "print(dash_line)\n",
    "print(\"Original model summary:\")\n",
    "print(original_model_text_outputs)\n",
    "print(dash_line)\n",
    "print(\"Instruct Model Summary:\")\n",
    "print(instruct_model_text_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece02191-7a3e-40d8-9758-c3eda4543618",
   "metadata": {},
   "source": [
    "Evaluate the model with Rouge metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ba24a93-3cf9-4ea9-8089-87bfba51522e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9b59de990d48cca9e243b6075306af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed586ae-d48f-4338-b106-19f4166db4a0",
   "metadata": {},
   "source": [
    "#Test for 10 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "535f444c-f738-4ef2-80c1-c00be433ffe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>#Person1#: Happy Birthday, Brian. #Person2#: T...</td>\n",
       "      <td>Brian's birthday is coming. Brian dances with ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  The memo is to be distributed to all employees...   \n",
       "1  The memo is to be distributed to all employees...   \n",
       "2  The memo is to be distributed to all employees...   \n",
       "3  The traffic jam at the Carrefour intersection ...   \n",
       "4  The traffic jam at the Carrefour intersection ...   \n",
       "5  The traffic jam at the Carrefour intersection ...   \n",
       "6               Masha and Hero are getting divorced.   \n",
       "7               Masha and Hero are getting divorced.   \n",
       "8               Masha and Hero are getting divorced.   \n",
       "9  #Person1#: Happy Birthday, Brian. #Person2#: T...   \n",
       "\n",
       "                            instruct_model_summaries  \n",
       "0  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "1  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "2  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "3  #Person2# got stuck in traffic again. #Person1...  \n",
       "4  #Person2# got stuck in traffic again. #Person1...  \n",
       "5  #Person2# got stuck in traffic again. #Person1...  \n",
       "6  Masha and Hero are getting divorced. Kate can'...  \n",
       "7  Masha and Hero are getting divorced. Kate can'...  \n",
       "8  Masha and Hero are getting divorced. Kate can'...  \n",
       "9  Brian's birthday is coming. Brian dances with ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dash_line = '-'.join(\"\" for x in range(100))\n",
    "\n",
    "dialogues = dataset[\"test\"][0:10][\"dialogue\"]\n",
    "human_baseline_summaries = dataset[\"test\"][0:10][\"summary\"]\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "for _,dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "    summarize the following conversation.\n",
    "    {dialogue}\n",
    "\n",
    "    summary:\"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt,return_tensors = 'pt').input_ids\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids = input_ids, generation_config = GenerationConfig(max_new_tokens = 200))\n",
    "    original_model_text_outputs = tokenizer.decode(original_model_outputs[0],skip_special_tokens = True)\n",
    "    original_model_summaries.append(original_model_text_outputs)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids = input_ids, generation_config = GenerationConfig(max_new_tokens = 200))\n",
    "    instruct_model_text_outputs = tokenizer.decode(instruct_model_outputs[0],skip_special_tokens = True)\n",
    "    instruct_model_summaries.append(instruct_model_text_outputs)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries,instruct_model_summaries))\n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d57aa688-4ebb-41e1-a4ca-dc5fddcbdca5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Scores:\n",
      "{'rouge1': 0.23972447825079402, 'rouge2': 0.10226305127006391, 'rougeL': 0.2046409608909609, 'rougeLsum': 0.20754735274472114}\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Instruct Model Scores:\n",
      "{'rouge1': 0.38881239922736843, 'rouge2': 0.17008025424479878, 'rougeL': 0.27974300959076787, 'rougeLsum': 0.27746115944731187}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions = original_model_summaries,\n",
    "    references = human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator = True,\n",
    "    use_stemmer = True\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions = instruct_model_summaries,\n",
    "    references = human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator = True,\n",
    "    use_stemmer = True\n",
    ")\n",
    "\n",
    "print(\"Original Model Scores:\")\n",
    "print(original_model_results)\n",
    "print(dash_line)\n",
    "print(\"Instruct Model Scores:\")\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a906a1f-4292-4868-ad1f-89cab9444e26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:14.91%\n",
      "rouge2:6.78%\n",
      "rougeL:7.51%\n",
      "rougeLsum:6.99%\n"
     ]
    }
   ],
   "source": [
    "improvement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(instruct_model_results.keys(), improvement):\n",
    "    print(f'{key}:{value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db24b988-b59a-4877-a6bb-701a0b09c1da",
   "metadata": {},
   "source": [
    "##PEFT Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0c5fc60-552c-4a2c-a515-ddf88210d928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 32,\n",
    "    lora_alpha= 32,\n",
    "    target_modules= ['q','v'],\n",
    "    lora_dropout = 0.05,\n",
    "    bias = 'none',\n",
    "    task_type = TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e261587a-6d4b-4492-a064-db35cba2ed4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Trainable model parameters : 3538944 \n",
      " All model parameters : 251116800 \n",
      " Percentage of trainable model parameters : 1.4092820552029972 \n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98c1463a-b91b-45e2-95be-14d0f84d0e19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = f'./peft-dialogue-summary-trainig_{str(int(time.time()))}'\n",
    "peft_training_args = TrainingArguments(\n",
    "            output_dir = output_dir,\n",
    "            auto_find_batch_size = True,\n",
    "            learning_rate = 1e-3,\n",
    "            num_train_epochs = 1,\n",
    "            logging_steps = 1,\n",
    "            max_steps = 1)\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model = peft_model,\n",
    "    args = peft_training_args,\n",
    "    train_dataset = tokenized_datasets[\"train\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a065e0a4-4ace-4895-8d79-82d8b73bfc5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>50.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n",
       " 'peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n",
       " 'peft-dialogue-summary-checkpoint-local/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path = f\"peft-dialogue-summary-checkpoint-local\"\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e703b980-6f95-4fb9-9b26-398241e943ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal error: An error occurred (IllegalLocationConstraintException) when calling the ListObjectsV2 operation: The unspecified location constraint is incompatible for the region specific endpoint this request was sent to.\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/ ./peft-dialogue-summary-checkpoint-from-s3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68bd4168-5705-45ca-872f-c9ab3618e5c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 14M Aug 19 20:30 ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f57f3938-d115-4b25-bd80-5ebec52284b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype = torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base,\n",
    "                                       './peft-dialogue-summary-checkpoint-from-s3',\n",
    "                                       torch_dtype = torch.bfloat16,\n",
    "                                       is_trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3168ff41-2466-4e77-9555-b26564d13ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Trainable model parameters : 0 \n",
      " All model parameters : 251116800 \n",
      " Percentage of trainable model parameters : 0.0 \n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "acfbcfe3-d804-4c91-ab14-d9c4d2c51f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "EXAMPLE  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Original model summary:\n",
      "#Pork1: Have you considered upgrading your system? #Person1: Yes, I'm not sure. #Pork2: You're right. #Pork3: I'm not sure. #Pork3: I'm not sure. #Pork4: I'm not sure. #Pork5: I'm not sure. #Pork4: I'm not sure. #Pork3: I'm not sure. #Pork4: I'm not sure. #Pork4: I'm not sure. #Pork3: I'm not sure. #Pork4: I'm not sure. #Pork4: I'm not sure. #Pork4: I'm not sure. #Pork3: I'm not sure. #Pork4: I\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Instruct Model Summary:\n",
      "#Person1# suggests #Person2# adding a painting program to #Person2#'s software and upgrading the hardware. #Person2# also wants to add a CD-ROM drive.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Peft Model Summary:\n",
      "#Person1# recommends adding a painting program to #Person2#'s software and upgrading hardware. #Person2# suggests adding a CD-ROM drive and #Person2# suggests adding a CD-ROM drive.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dash_line = '-'.join(\"\" for x in range(100))\n",
    "\n",
    "dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "human_summary = dataset[\"test\"][index][\"summary\"]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "summarize the following conversation.\n",
    "{dialogue}\n",
    "\n",
    "summary:\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt,return_tensors = 'pt').input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids = input_ids, generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1))\n",
    "original_model_text_outputs = tokenizer.decode(original_model_outputs[0],skip_special_tokens = True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids = input_ids, generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1))\n",
    "instruct_model_text_outputs = tokenizer.decode(instruct_model_outputs[0],skip_special_tokens = True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids = input_ids, generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1))\n",
    "peft_model_text_outputs = tokenizer.decode(peft_model_outputs[0],skip_special_tokens = True)\n",
    "\n",
    "print(dash_line)\n",
    "print(\"EXAMPLE \", i+1)\n",
    "print(dash_line)\n",
    "print(\"Human Summary:\")\n",
    "print(human_summary)\n",
    "print(dash_line)\n",
    "print(\"Original model summary:\")\n",
    "print(original_model_text_outputs)\n",
    "print(dash_line)\n",
    "print(\"Instruct Model Summary:\")\n",
    "print(instruct_model_text_outputs)\n",
    "print(dash_line)\n",
    "print(\"Peft Model Summary:\")\n",
    "print(peft_model_text_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bea66e98-fa16-4976-9308-f2e286ef586f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>The memo is a dictation for the office.</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>Employees are being required to take a dictati...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>This memo is a memo to all employees in the of...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>Person1: You're finally here!</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "      <td>#Person2# got stuck in traffic and got stuck i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>The person has been waiting for a long time.</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "      <td>#Person2# got stuck in traffic and got stuck i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>The car is causing a traffic jam at work.</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "      <td>#Person2# got stuck in traffic and got stuck i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are getting a divorce.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
       "      <td>Kate tells #Person2# Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are having a divorce.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
       "      <td>Kate tells #Person2# Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
       "      <td>Kate tells #Person2# Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>#Porce1: Brian, thank you for coming to our pa...</td>\n",
       "      <td>Brian's birthday is coming. Brian dances with ...</td>\n",
       "      <td>Brian remembers his birthday and invites #Pers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0            The memo is a dictation for the office.   \n",
       "1  Employees are being required to take a dictati...   \n",
       "2  This memo is a memo to all employees in the of...   \n",
       "3                      Person1: You're finally here!   \n",
       "4       The person has been waiting for a long time.   \n",
       "5          The car is causing a traffic jam at work.   \n",
       "6              Masha and Hero are getting a divorce.   \n",
       "7               Masha and Hero are having a divorce.   \n",
       "8               Masha and Hero are getting divorced.   \n",
       "9  #Porce1: Brian, thank you for coming to our pa...   \n",
       "\n",
       "                            instruct_model_summaries  \\\n",
       "0  #Person1# asks Ms. Dawson to take a dictation ...   \n",
       "1  #Person1# asks Ms. Dawson to take a dictation ...   \n",
       "2  #Person1# asks Ms. Dawson to take a dictation ...   \n",
       "3  #Person2# got stuck in traffic again. #Person1...   \n",
       "4  #Person2# got stuck in traffic again. #Person1...   \n",
       "5  #Person2# got stuck in traffic again. #Person1...   \n",
       "6  Masha and Hero are getting divorced. Kate can'...   \n",
       "7  Masha and Hero are getting divorced. Kate can'...   \n",
       "8  Masha and Hero are getting divorced. Kate can'...   \n",
       "9  Brian's birthday is coming. Brian dances with ...   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "1  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "2  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "3  #Person2# got stuck in traffic and got stuck i...  \n",
       "4  #Person2# got stuck in traffic and got stuck i...  \n",
       "5  #Person2# got stuck in traffic and got stuck i...  \n",
       "6  Kate tells #Person2# Masha and Hero are gettin...  \n",
       "7  Kate tells #Person2# Masha and Hero are gettin...  \n",
       "8  Kate tells #Person2# Masha and Hero are gettin...  \n",
       "9  Brian remembers his birthday and invites #Pers...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dash_line = '-'.join(\"\" for x in range(100))\n",
    "\n",
    "dialogues = dataset[\"test\"][0:10][\"dialogue\"]\n",
    "human_baseline_summaries = dataset[\"test\"][0:10][\"summary\"]\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for _,dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "    summarize the following conversation.\n",
    "    {dialogue}\n",
    "\n",
    "    summary:\"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt,return_tensors = 'pt').input_ids\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids = input_ids, generation_config = GenerationConfig(max_new_tokens = 200))\n",
    "    original_model_text_outputs = tokenizer.decode(original_model_outputs[0],skip_special_tokens = True)\n",
    "    original_model_summaries.append(original_model_text_outputs)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids = input_ids, generation_config = GenerationConfig(max_new_tokens = 200))\n",
    "    instruct_model_text_outputs = tokenizer.decode(instruct_model_outputs[0],skip_special_tokens = True)\n",
    "    instruct_model_summaries.append(instruct_model_text_outputs)\n",
    "    \n",
    "    peft_model_outputs = peft_model.generate(input_ids = input_ids, generation_config = GenerationConfig(max_new_tokens = 200))\n",
    "    peft_model_text_outputs = tokenizer.decode(peft_model_outputs[0],skip_special_tokens = True)\n",
    "    peft_model_summaries.append(peft_model_text_outputs)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries,instruct_model_summaries, peft_model_summaries))\n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d95434fd-4b3f-482d-9e44-9f9e53bbe62f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Scores:\n",
      "{'rouge1': 0.23405506287085232, 'rouge2': 0.07366666666666669, 'rougeL': 0.21329166934430094, 'rougeLsum': 0.21648051753314912}\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Instruct Model Scores:\n",
      "{'rouge1': 0.38881239922736843, 'rouge2': 0.17008025424479878, 'rougeL': 0.27974300959076787, 'rougeLsum': 0.27746115944731187}\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Peft Model Scores:\n",
      "{'rouge1': 0.3636453677458462, 'rouge2': 0.12664656829900028, 'rougeL': 0.2703710325071955, 'rougeLsum': 0.2701132504457793}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions = original_model_summaries,\n",
    "    references = human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator = True,\n",
    "    use_stemmer = True\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions = instruct_model_summaries,\n",
    "    references = human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator = True,\n",
    "    use_stemmer = True\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions = peft_model_summaries,\n",
    "    references = human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator = True,\n",
    "    use_stemmer = True\n",
    ")\n",
    "\n",
    "print(\"Original Model Scores:\")\n",
    "print(original_model_results)\n",
    "print(dash_line)\n",
    "print(\"Instruct Model Scores:\")\n",
    "print(instruct_model_results)\n",
    "print(dash_line)\n",
    "print(\"Peft Model Scores:\")\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52ac6112-0f84-4bd7-b630-dc63edfff17c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!touch ./.gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d42f29-8188-45cb-9baf-8df2e94f59cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:me-south-1:117516905037:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
